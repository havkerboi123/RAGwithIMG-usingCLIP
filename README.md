![clip.png]

Explored multi-modal RAG with images using OpenAI's CLIP for embeddings with langChain for retrieval along with gpt4 vision.

CLIP works on a simple pre-training task of predicting which caption goes with which image, an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts enabling zero-shot transfer of the model to downstream tasks. 
